{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bbe63cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 300/300 [00:16<00:00, 18.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2  # 导入OpenCV库用于视频处理\n",
    "\n",
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )\n",
    "\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = \"D:\\HDU\\STORE\\sam2\\sam2.1_hiera_tiny.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
    "\n",
    "# 用于显示掩码、点和框的辅助函数\n",
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n",
    "\n",
    "# 视频目录设置\n",
    "video_dir = \"D:\\HDU\\STORE\\sam2/notebooks/videos\\demo\"\n",
    "\n",
    "# 扫描所有JPEG帧文件名\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# 初始化推理状态\n",
    "inference_state = predictor.init_state(video_path=video_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ba2bfc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 300/300 [00:13<00:00, 22.44it/s]\n",
      "propagate in video: 100%|██████████| 300/300 [00:51<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame dimensions: width = 1280, height = 720\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2  # 导入OpenCV库用于视频处理\n",
    "\n",
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )\n",
    "\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = \"D:\\HDU\\STORE\\sam2\\sam2.1_hiera_tiny.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
    "\n",
    "# 用于显示掩码、点和框的辅助函数\n",
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n",
    "\n",
    "# 视频目录设置\n",
    "video_dir = \"D:\\HDU\\STORE\\sam2/notebooks/videos\\demo\"\n",
    "\n",
    "# 扫描所有JPEG帧文件名\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "# 初始化推理状态\n",
    "inference_state = predictor.init_state(video_path=video_dir)\n",
    "\n",
    "# 用于存储点击点的字典\n",
    "prompts = {}\n",
    "\n",
    "# 处理第一个对象\n",
    "ann_frame_idx = 0\n",
    "ann_obj_id_yaocong = 2\n",
    "def pro_1_with_point():\n",
    "    points = np.array([[770, 370], [700, 390],[730,340],[820,360]], dtype=np.float32)  # 添加负点击点\n",
    "    labels = np.array([1, 0, 0, 0], dtype=np.int32)  # 1 表示正点击，0 表示负点击\n",
    "    prompts[ann_obj_id_yaocong] = points, labels\n",
    "\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_obj_id_yaocong,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "def pro_1_with_box():\n",
    "\n",
    "    # Let's add a box at (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) to get started\n",
    "    box = np.array([740, 350, 800, 400], dtype=np.float32)\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_obj_id_yaocong,\n",
    "        box=box,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def pro_2_with_point():\n",
    "    \n",
    "    ann_frame_idx = 0\n",
    "    ann_obj_id_hengtu = 3\n",
    "\n",
    "    points = np.array([[535,450],[575, 400],[575,330],[575,400],[575, 375],[625, 395],[525, 375]], dtype=np.float32)\n",
    "    # for labels, `1` means positive click and `0` means negative click\n",
    "    labels = np.array([0,0,0,0,1,0,0], np.int32)\n",
    "    prompts[ann_obj_id_hengtu] = points, labels\n",
    "\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_obj_id_hengtu,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "\n",
    "ann_obj_id_hengtu = 3 \n",
    "def pro_2_with_box():\n",
    "    ann_frame_idx = 0  # the frame index we interact with\n",
    "      # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "    # Let's add a box at (x_min, y_min, x_max, y_max) = (300, 0, 500, 400) to get started\n",
    "    box = np.array([540, 350, 600, 400], dtype=np.float32)\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_obj_id_hengtu,\n",
    "        box=box,\n",
    "    )\n",
    "# pro_1_with_box()\n",
    "# pro_2_with_box()\n",
    "pro_1_with_point()\n",
    "pro_2_with_point()\n",
    "pro_2_with_point()\n",
    "\n",
    "# 在整个视频中传播分割结果\n",
    "video_segments = {}\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "\n",
    "# 获取视频帧的尺寸信息\n",
    "sample_frame = Image.open(os.path.join(video_dir, frame_names[0]))\n",
    "frame_height, frame_width = sample_frame.height, sample_frame.width\n",
    "\n",
    "# 打印帧的尺寸信息以进行调试\n",
    "print(f\"Frame dimensions: width = {frame_width}, height = {frame_height}\")\n",
    "\n",
    "# 设置视频编码器和输出文件\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out_video = cv2.VideoWriter('output-pp.mp4', fourcc, 25.0, (frame_width, frame_height))\n",
    "\n",
    "# 遍历每一帧并生成带有分割结果的视频\n",
    "for out_frame_idx in range(len(frame_names)):\n",
    "    frame_path = os.path.join(video_dir, frame_names[out_frame_idx])\n",
    "    frame = np.array(Image.open(frame_path))\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # 转换颜色空间以适应OpenCV\n",
    "\n",
    "    if out_frame_idx in video_segments:\n",
    "        for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "            # 确保掩码的形状与帧的形状匹配\n",
    "            out_mask = out_mask.astype(np.bool_)\n",
    "            out_mask_expanded = np.expand_dims(out_mask, axis=-1)  # 扩展掩码维度以匹配帧的形状\n",
    "\n",
    "            # # 将掩码调整到与帧相同的大小\n",
    "            # print(f\"Mask shape: {out_mask_expanded.shape}, Frame shape: {(frame_height, frame_width)}\")\n",
    "            if out_mask_expanded.shape[1:3] != (frame_height, frame_width):\n",
    "                out_mask_resized = cv2.resize(\n",
    "                    out_mask.astype(np.uint8),\n",
    "                    (frame_width, frame_height),\n",
    "                    interpolation=cv2.INTER_NEAREST\n",
    "                )\n",
    "                out_mask_resized = np.expand_dims(out_mask_resized, axis=-1)\n",
    "            else:\n",
    "                out_mask_resized = out_mask_expanded\n",
    "\n",
    "            # 将掩码应用到帧上\n",
    "            mask_color = np.array([0, 255, 0])  # 绿色掩码\n",
    "            if out_obj_id == ann_obj_id_yaocong:\n",
    "                mask_color = np.array([0, 0, 255])  # 红色掩码用于第一个对象\n",
    "            elif out_obj_id == ann_obj_id_hengtu:\n",
    "                mask_color = np.array([255, 0, 0])  # 蓝色掩码用于第二个对象\n",
    "\n",
    "            # print(f\"Mask shape after resizing: {out_mask_resized.shape}\")\n",
    "            out_mask_resized = out_mask_resized.squeeze(0)\n",
    "            out_mask_resized = out_mask_resized.squeeze(-1)\n",
    "            \n",
    "            frame[out_mask_resized] = frame[out_mask_resized] * 0.7 + mask_color * 0.3  # 半透明显示\n",
    "\n",
    "    out_video.write(frame)\n",
    "\n",
    "# 释放视频写入器\n",
    "out_video.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "94",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
